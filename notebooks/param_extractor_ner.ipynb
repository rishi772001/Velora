{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c2056",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------- 1. Install required libraries --------------------\n",
    "!pip install transformers datasets seqeval --quiet\n",
    "\n",
    "# -------------------- 2. Prepare your label list --------------------\n",
    "label_list = [\"O\", \"B-from\", \"B-to\", \"B-date\", \"I-date\", \"B-count\"]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# -------------------- 3. Sample data --------------------\n",
    "from datasets import Dataset\n",
    "\n",
    "def conll_to_data(filepath):\n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "\n",
    "    all_tokens = []\n",
    "    all_tags = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if all_tokens:\n",
    "                    tokens.append(all_tokens)\n",
    "                    ner_tags.append([label_to_id[tag] for tag in all_tags])\n",
    "                    all_tokens = []\n",
    "                    all_tags = []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                if len(splits) >= 2:\n",
    "                    token, tag = splits[0], splits[1]\n",
    "                    all_tokens.append(token)\n",
    "                    all_tags.append(tag)\n",
    "                else:\n",
    "                    # Skip malformed lines\n",
    "                    continue\n",
    "        # Catch last sentence if file doesn't end with newline\n",
    "        if all_tokens:\n",
    "            tokens.append(all_tokens)\n",
    "            ner_tags.append([label_to_id[tag] for tag in all_tags])\n",
    "\n",
    "    data = {\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": ner_tags\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# Token list and label IDs for each token\n",
    "data = conll_to_data(\"train.txt\")\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# -------------------- 4. Load tokenizer and model --------------------\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "\n",
    "# -------------------- 5. Tokenize and align labels --------------------\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])  # or use I-... logic if needed\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "# -------------------- 6. Training setup --------------------\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_preds = [\n",
    "        [id_to_label[p] for (p, l) in zip(pred_row, label_row) if l != -100]\n",
    "        for pred_row, label_row in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id_to_label[l] for (p, l) in zip(pred_row, label_row) if l != -100]\n",
    "        for pred_row, label_row in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\"f1\": f1_score(true_labels, true_preds), \"report\": classification_report(true_labels, true_preds)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# -------------------- 7. Train the model --------------------\n",
    "trainer.train()\n",
    "\n",
    "# -------------------- 8. Test prediction --------------------\n",
    "def predict(sentence):\n",
    "    tokens = sentence.split()\n",
    "    inputs = tokenizer(tokens, return_tensors=\"pt\", is_split_into_words=True, truncation=True)\n",
    "    outputs = model(**inputs).logits\n",
    "    predictions = outputs.argmax(dim=2)[0].tolist()\n",
    "    print(\"\\nPredictions:\")\n",
    "    for token, pred_id in zip(tokens, predictions[1:len(tokens)+1]):\n",
    "        print(f\"{token}: {id_to_label[pred_id]}\")\n",
    "\n",
    "# Example test\n",
    "predict(\"Book a flight to mumbai from kolkata on July 5 for 3 people\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b3b24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as parse_date\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_slots_from_tokens(tokens, labels):\n",
    "    slots = {\n",
    "        \"from\": \"\",\n",
    "        \"to\": \"\",\n",
    "        \"date\": \"\",\n",
    "        \"count\": \"\"\n",
    "    }\n",
    "\n",
    "    current_entity = None\n",
    "    buffer = []\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity and buffer:\n",
    "                value = \" \".join(buffer)\n",
    "                slots[current_entity] = value\n",
    "                buffer = []\n",
    "            current_entity = label[2:].lower()\n",
    "            buffer.append(token)\n",
    "        elif label.startswith(\"I-\") and current_entity:\n",
    "            buffer.append(token)\n",
    "        else:\n",
    "            if current_entity and buffer:\n",
    "                value = \" \".join(buffer)\n",
    "                slots[current_entity] = value\n",
    "                buffer = []\n",
    "                current_entity = None\n",
    "\n",
    "    # Capture the last entity\n",
    "    if current_entity and buffer:\n",
    "        value = \" \".join(buffer)\n",
    "        slots[current_entity] = value\n",
    "\n",
    "    # Post-process date\n",
    "    if slots[\"date\"]:\n",
    "        try:\n",
    "            parsed_date = parse_date(slots[\"date\"], fuzzy=True, dayfirst=True)\n",
    "            slots[\"date\"] = parsed_date.strftime(\"%d/%m/%Y\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return slots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a399fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Updated predict with response parsing\n",
    "def predict(sentence):\n",
    "    tokens = sentence.split()\n",
    "    inputs = tokenizer(tokens, return_tensors=\"pt\", is_split_into_words=True, truncation=True)\n",
    "    outputs = model(**inputs).logits\n",
    "    predictions = outputs.argmax(dim=2)[0].tolist()[1:len(tokens)+1]\n",
    "    labels = [id_to_label[pred_id] for pred_id in predictions]\n",
    "\n",
    "    print(\"\\nToken-level Predictions:\")\n",
    "    for token, label in zip(tokens, labels):\n",
    "        print(f\"{token}: {label}\")\n",
    "\n",
    "    structured_output = extract_slots_from_tokens(tokens, labels)\n",
    "    print(\"\\nExtracted Slots:\")\n",
    "    print(structured_output)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
